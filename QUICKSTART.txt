â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                           â•‘
â•‘           ğŸš€ MoEl - Quick Start Guide ğŸš€                  â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INSTALLATION (3 MINUTES)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Extract the archive:
   
   tar -xzf moel.tar.gz
   cd moel

2. Run the setup:
   
   chmod +x run.sh
   ./run.sh setup

   During setup, you'll be asked to:
   - Choose GPU acceleration (if available)
   - Select an LLM model (GPT-2 recommended for testing)

3. Start MoEl:
   
   ./run.sh start

4. Open your browser:
   
   http://localhost:5000


USAGE EXAMPLES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Single File Analysis:
  1. Go to http://localhost:5000
  2. Select task type (e.g., "Code Generation")
  3. Enter your prompt
  4. Click "Generate"

Batch Processing:
  1. Select "Batch Processing" tab
  2. Upload multiple files
  3. Set default prompt template (optional)
  4. Click "Process Files"

Test with Examples:
  - uploads/example_code_gen.txt
  - uploads/example_code_review.py
  - uploads/example_translation.txt


COMMANDS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

./run.sh setup     - Initial installation
./run.sh start     - Start the application
./run.sh stop      - Stop all services
./run.sh restart   - Restart services
./run.sh status    - Check if running
./run.sh logs      - View live logs


TROUBLESHOOTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Backend won't start:
  â†’ Check logs: cat logs/backend_*.log
  â†’ Verify model downloaded properly
  â†’ Ensure port 8000 is available

Frontend can't connect:
  â†’ Verify backend is running: curl http://localhost:8000/health
  â†’ Check logs: cat logs/frontend_*.log

Out of memory:
  â†’ Use smaller model (GPT-2 instead of GPT-2 Large)
  â†’ Switch to CPU mode if GPU memory is low
  â†’ Close other applications


HARDWARE RECOMMENDATIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CPU Only:
  - GPT-2 (500MB) âœ“ Fast
  - GPT-2 Medium (1.5GB) âœ“ Good
  - GPT-2 Large (3GB) - Slow but works

GPU (6GB+ VRAM):
  - GPT-2 XL (6GB) âœ“ Excellent
  - Any 7B model âœ“ Production ready
  - 13B models âœ“ High quality

GPU (8GB+ VRAM):
  - 13B models âœ“ Excellent
  - 20B models âœ“ Best quality


API ACCESS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Single inference:
  POST http://localhost:8000/inference
  {
    "prompt": "Write a Python function",
    "max_tokens": 512,
    "temperature": 0.7,
    "task_type": "code_gen"
  }

Batch processing:
  POST http://localhost:8000/upload-and-process
  (multipart/form-data with files)


NEXT STEPS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ Read the complete README.md for advanced features
âœ“ Try different models from HuggingFace
âœ“ Customize prompts for your use case
âœ“ Integrate with your applications via API
âœ“ Scale up to larger models as needed


SUPPORT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

For detailed documentation, see README.md
For issues, check the logs/ directory
For configuration, edit config/hardware.json

Happy coding! ğŸ‰
