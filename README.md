# üöÄ MoEl - High-Performance Local LLM Platform

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green.svg)
![Flask](https://img.shields.io/badge/Flask-Frontend-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Run powerful AI models locally with a beautiful web interface**

[Quick Start](#-quick-start) ‚Ä¢ [Models](#-recommended-models) ‚Ä¢ [Configuration](#%EF%B8%8F-configuration) ‚Ä¢ [API](#-api-reference)

</div>

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| üéØ **Single File Analysis** | Process individual files with custom prompts |
| üì¶ **Batch Processing** | Upload and process multiple files simultaneously |
| ü§ñ **Task Types** | Code Generation, Code Review, Translation, Summarization |
| ‚ö° **CPU Optimized** | Fast inference even without GPU |
| üéÆ **GPU Acceleration** | NVIDIA CUDA, AMD ROCm, Apple MPS support |
| üß† **NPU Support** | Intel/Qualcomm NPU acceleration |
| üåê **Web Interface** | Modern, responsive UI at `localhost:5000` |
| üîå **REST API** | Programmatic access at `localhost:8000` |

---

## üìã System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **OS** | Windows 10, Ubuntu 20.04, macOS 10.15 | Latest versions |
| **Python** | 3.8 | 3.10+ |
| **RAM** | 4GB | 8GB+ |
| **Storage** | 2GB | 5GB+ |
| **CPU** | 2 cores | 4+ cores |

---

## üéØ Recommended Models

### üíª CPU (No GPU Required)

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **`Qwen/Qwen2.5-0.5B-Instruct`** ‚≠ê | 1GB | ‚ö°‚ö°‚ö° Fast | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Default, general use |
| `Qwen/Qwen2.5-1.5B-Instruct` | 3GB | ‚ö°‚ö° Medium | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Better quality |
| `microsoft/phi-2` | 5GB | ‚ö° Slow | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Best quality on CPU |
| `gpt2` | 500MB | ‚ö°‚ö°‚ö°‚ö° Very Fast | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ | Testing only |

### üéÆ GPU (NVIDIA CUDA / AMD ROCm)

| Model | VRAM | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **`Qwen/Qwen2.5-1.5B-Instruct`** ‚≠ê | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Default GPU choice |
| `Qwen/Qwen2.5-3B-Instruct` | 6GB | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | High quality |
| `Qwen/Qwen2.5-7B-Instruct` | 8GB | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Professional use |
| `microsoft/phi-2` | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Excellent balance |
| `mistralai/Mistral-7B-Instruct-v0.2` | 8GB | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Top quality |

### üçé Apple Silicon (MPS)

| Model | Memory | Speed | Quality | Best For |
|-------|--------|-------|---------|----------|
| **`Qwen/Qwen2.5-1.5B-Instruct`** ‚≠ê | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | M1/M2/M3 default |
| `Qwen/Qwen2.5-3B-Instruct` | 6GB | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | M1 Pro/Max |
| `microsoft/phi-2` | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | All Apple Silicon |

### üß† NPU (Intel/Qualcomm Neural Processors)

| Model | Memory | Speed | Quality | Best For |
|-------|--------|-------|---------|----------|
| **`Qwen/Qwen2.5-0.5B-Instruct`** ‚≠ê | 1GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | Intel Core Ultra |
| `Qwen/Qwen2.5-1.5B-Instruct` | 3GB | ‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | Snapdragon X Elite |
| `microsoft/phi-2` | 5GB | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | NPU with 8GB+ RAM |

> üí° **Tip**: Start with `Qwen/Qwen2.5-0.5B-Instruct` - it's fast, accurate, and works on any hardware!

---

## üöÄ Quick Start

### Windows (PowerShell)

```powershell
# 1. Clone the repository
git clone https://github.com/ANOUARELIDRISSI/MoEl-Local.git
cd MoEl-Local

# 2. Run setup (installs dependencies + downloads model)
.\run.ps1 setup

# 3. Start the application
.\run.ps1 start

# 4. Open http://localhost:5000 in your browser
```

### Linux / macOS (Bash)

```bash
# 1. Clone the repository
git clone https://github.com/ANOUARELIDRISSI/MoEl-Local.git
cd MoEl-Local

# 2. Make script executable
chmod +x run.sh

# 3. Run setup
./run.sh setup

# 4. Start the application
./run.sh start

# 5. Open http://localhost:5000 in your browser
```

### Cross-Platform (Python)

```bash
# Works on any system with Python 3.8+
python setup.py setup
python setup.py start
```

---

## üìñ How to Run & Use

### Step 1: Setup (First Time Only)

The setup process will:
- Create a Python virtual environment
- Install all required dependencies
- Download the default AI model (~1GB)

```bash
# Windows
.\run.ps1 setup

# Linux/Mac
./run.sh setup

# Or using Python directly
python setup.py setup
```

‚è±Ô∏è **Setup takes 3-10 minutes** depending on your internet speed.

### Step 2: Start the Application

```bash
# Windows
.\run.ps1 start

# Linux/Mac
./run.sh start

# Or using Python
python setup.py start
```

This will:
1. Start the **Backend API** on port `8000`
2. Start the **Web Interface** on port `5000`
3. Open your browser automatically

### Step 3: Use the Web Interface

Once running, open **http://localhost:5000** in your browser:

#### Single Prompt Mode
1. Select a **Task Type** (General, Code Generation, Code Review, Translation, Summarization)
2. Enter your **prompt** in the text area
3. Adjust settings (optional):
   - **Max Tokens**: Length of response (default: 512)
   - **Temperature**: Creativity level 0.0-1.0 (default: 0.7)
4. Click **Generate**
5. View the AI response

#### Batch Processing Mode
1. Click **"Batch Processing"** tab
2. Upload multiple files (.txt, .py, .js, .md, etc.)
3. Set a **default prompt template** (use `{content}` as placeholder)
   - Example: `"Review this code and suggest improvements: {content}"`
4. Click **Process Files**
5. View results for each file

### Step 4: Stop the Application

```bash
# Windows
.\run.ps1 stop

# Linux/Mac
./run.sh stop

# Or using Python
python setup.py stop
```

### Example Usage

**Code Generation:**
```
Prompt: "Write a Python function to find prime numbers up to n"
Task: Code Generation
```

**Code Review:**
```
Prompt: "Review this code for bugs and improvements"
Task: Code Review
[Upload your code file]
```

**Translation:**
```
Prompt: "Translate this text to French: Hello, how are you?"
Task: Translation
```

---

## üéÆ Commands Reference

| Action | Windows | Linux/Mac | Python |
|--------|---------|-----------|--------|
| **Setup** | `.\run.ps1 setup` | `./run.sh setup` | `python setup.py setup` |
| **Start** | `.\run.ps1 start` | `./run.sh start` | `python setup.py start` |
| **Stop** | `.\run.ps1 stop` | `./run.sh stop` | `python setup.py stop` |
| **Restart** | `.\run.ps1 restart` | `./run.sh restart` | `python setup.py restart` |
| **Status** | `.\run.ps1 status` | `./run.sh status` | `python setup.py status` |
| **Logs** | `.\run.ps1 logs` | `./run.sh logs` | - |

---

## ‚öôÔ∏è Configuration

### Hardware Configuration (`config/hardware.json`)

```json
{
  "device": "cpu",
  "gpu_type": "none",
  "gpu_layers": 0,
  "threads": 4,
  "context_length": 2048,
  "model_name": "Qwen/Qwen2.5-0.5B-Instruct"
}
```

### Configuration Options

| Option | Values | Description |
|--------|--------|-------------|
| `device` | `cpu`, `cuda`, `mps`, `npu` | Compute device |
| `gpu_type` | `none`, `nvidia`, `amd`, `mps`, `intel_npu` | Hardware type |
| `gpu_layers` | `0` - `35` | Layers offloaded to accelerator |
| `threads` | `1` - `16` | CPU threads for inference |
| `context_length` | `512` - `8192` | Max context window |
| `model_name` | HuggingFace ID | Model identifier |

### Device Configuration Examples

<details>
<summary><b>üíª CPU Only</b></summary>

```json
{
  "device": "cpu",
  "gpu_type": "none",
  "gpu_layers": 0,
  "threads": 8,
  "context_length": 2048,
  "model_name": "Qwen/Qwen2.5-0.5B-Instruct"
}
```
</details>

<details>
<summary><b>üéÆ NVIDIA GPU</b></summary>

```json
{
  "device": "cuda",
  "gpu_type": "nvidia",
  "gpu_layers": 35,
  "threads": 4,
  "context_length": 4096,
  "model_name": "Qwen/Qwen2.5-1.5B-Instruct"
}
```
</details>

<details>
<summary><b>üçé Apple Silicon</b></summary>

```json
{
  "device": "mps",
  "gpu_type": "mps",
  "gpu_layers": 35,
  "threads": 8,
  "context_length": 4096,
  "model_name": "Qwen/Qwen2.5-1.5B-Instruct"
}
```
</details>

<details>
<summary><b>üß† Intel NPU (Core Ultra)</b></summary>

```json
{
  "device": "npu",
  "gpu_type": "intel_npu",
  "gpu_layers": 0,
  "threads": 8,
  "context_length": 2048,
  "model_name": "Qwen/Qwen2.5-0.5B-Instruct"
}
```
</details>

---

## üîå API Reference

### Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check & system info |
| `/inference` | POST | Single prompt inference |
| `/batch-inference` | POST | Batch processing |
| `/upload-and-process` | POST | File upload & processing |

### Health Check

```bash
curl http://localhost:8000/health
```

```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cpu",
  "model_name": "Qwen/Qwen2.5-0.5B-Instruct"
}
```

### Single Inference

```bash
curl -X POST http://localhost:8000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a Python function to calculate fibonacci",
    "max_tokens": 512,
    "temperature": 0.7,
    "task_type": "code_gen"
  }'
```

### Batch Inference

```bash
curl -X POST http://localhost:8000/batch-inference \
  -H "Content-Type: application/json" \
  -d '{
    "prompts": ["Explain Python lists", "What is recursion?"],
    "max_tokens": 256,
    "temperature": 0.7
  }'
```

---

## üìÅ Project Structure

```
moel/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îî‚îÄ‚îÄ app.py              # FastAPI backend server
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Flask frontend server
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html      # Web interface
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ hardware.json       # Hardware configuration (local)
‚îÇ   ‚îî‚îÄ‚îÄ hardware.json.example
‚îú‚îÄ‚îÄ logs/                   # Application logs
‚îú‚îÄ‚îÄ uploads/                # Uploaded files
‚îú‚îÄ‚îÄ outputs/                # Generated outputs
‚îú‚îÄ‚îÄ run.ps1                 # Windows launcher
‚îú‚îÄ‚îÄ run.sh                  # Linux/Mac launcher
‚îú‚îÄ‚îÄ setup.py                # Cross-platform launcher
‚îî‚îÄ‚îÄ requirements.txt        # Python dependencies
```

---

## üîß Troubleshooting

<details>
<summary><b>Port Already in Use</b></summary>

```powershell
# Windows
Get-NetTCPConnection -LocalPort 8000 | ForEach-Object { Stop-Process -Id $_.OwningProcess -Force }
```

```bash
# Linux/Mac
lsof -ti:8000 | xargs kill -9
```
</details>

<details>
<summary><b>Model Loading Slow</b></summary>

First startup downloads the model (~1GB). Subsequent starts use the cached model from `~/.cache/huggingface/`.
</details>

<details>
<summary><b>Out of Memory</b></summary>

- Use a smaller model (e.g., `gpt2` or `Qwen2.5-0.5B`)
- Reduce `context_length` in config
- Close other applications
</details>

<details>
<summary><b>Check Logs</b></summary>

```bash
# Windows
Get-Content .\logs\backend_*.log -Tail 50

# Linux/Mac
tail -50 logs/backend.log
```
</details>

---

## üìÑ License

MIT License - feel free to use, modify, and distribute.

---

<div align="center">

**Made with ‚ù§Ô∏è for local AI inference**

[‚¨Ü Back to Top](#-moel---high-performance-local-llm-platform)

</div>
